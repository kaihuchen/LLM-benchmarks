# Open-source Benchmarks for LMMs (Large Multimodal Models)

This repository is meant to be the home for a collection of many collections of datasets for testing the vision performance of a LMM.

## Background

LMMs have become practical as a new tool for performing visual recognition from an image, with the ability to identify multiple objects, their relationship, the environment, assessment of the overall situation per a certain given goal, suggest the best course of commonsense actions, and explain the actions.

For example, in the context of level 5 autonomous driving, a single test case might look like the following:

- Image: ![tornado](images/tornado2.jpg)
- Description: a large tornado ahead
- Action: turn around and evacuate
- Explanation: Tornadoes are incredibly dangerous and can cause major damage and injuries, even if you are not directly hit by the funnel cloud.

*(to be filled)*

## Related Resources

Working documents for collected test cases:

- For [level 5 autonomous driving](https://github.com/kaihuchen/AutonomousBackseatDriver)
- For a *home guardian*, a mobile home robot with the duty to identify any hazard in and around a home.
*(to be filled)* 

## To the Community

You are welcome to contribute to these dataset. If you have any feedbacks please post them to the *issues* area of this repo.

